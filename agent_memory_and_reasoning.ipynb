{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u26puzb3HyT9",
        "outputId": "e3c69f09-c8f8-4067-a8b3-32bcb21498a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/200.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.0/200.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "#step 1: install/upgrade the latest genai SDK\n",
        "%pip install google-genai --upgrade --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import the genai library\n",
        "from google import genai"
      ],
      "metadata": {
        "id": "_jqkjaSLSGUB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#step2: AIStudio: read the api key from the user data\n",
        "from google.colab import userdata\n",
        "client = genai.Client(api_key=userdata.get(\"GEMINI_KEY\"))\n",
        "\n",
        "#If you want to read from environment keys\n",
        "#import os\n",
        "#client = genai.Client(api_key=os.environ[\"GEMINI_API_KEY\"])"
      ],
      "metadata": {
        "id": "NRmQ0FpeSSnL"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use a Gemini 2.5 model supporting chat\n",
        "# model = genai.GenerativeModel('gemini-2.5-flash-preview-04-17')\n",
        "model_name = \"gemini-2.5-flash-preview-04-17\"\n",
        "# chat = client.models.start_chat(history=[]) # Start with empty history\n",
        "chat = client.chats.create(\n",
        "    model=model_name,\n",
        "    history=[],\n",
        ")\n",
        "\n",
        "# First turn\n",
        "response = chat.send_message(\"Explain context caching.\")\n",
        "print(response.text)\n",
        "\n",
        "# Second turn - history is automatically included\n",
        "response = chat.send_message(\"When should I use it?\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CdAkT-zl6ZTa",
        "outputId": "07f29e91-d81c-45f1-f9dc-9b4acd119688"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Okay, let's break down **Context Caching** in the context of Large Language Models (LLMs).\n",
            "\n",
            "In essence, context caching is a technique used to improve the speed and efficiency of LLMs, particularly when dealing with sequences of interactions (like a chat conversation) or generating long output sequences.\n",
            "\n",
            "Here's a more detailed explanation:\n",
            "\n",
            "1.  **The Problem: Repetitive Computation**\n",
            "    *   LLMs process input sequences token by token. When you provide a prompt or continue a conversation, the model takes the *entire* current input sequence (previous turns + the new message) and processes it through its layers to generate the next token.\n",
            "    *   A critical and often computationally expensive part of this process is the **Attention Mechanism**. The attention mechanism calculates how much importance each token in the input sequence should give to every other token in the sequence.\n",
            "    *   For self-attention within layers, this involves calculating Key (K) and Value (V) vectors for each token. To generate the output for a *new* token, the model needs to compute how it attends to *all* previous tokens in the current input sequence.\n",
            "    *   Without caching, every time you add a new token (either in a new user turn or as the model generates its own response token by token), the model would have to re-process the *entire* preceding sequence from scratch, re-calculating the K and V vectors for tokens it has already seen and processed many times. This is redundant and slow.\n",
            "\n",
            "2.  **The Solution: Cache the Keys and Values (KV Cache)**\n",
            "    *   Context caching, specifically **KV Caching**, tackles this redundancy.\n",
            "    *   When the model processes an input sequence for the first time (e.g., your initial prompt or the first turn of a conversation), it calculates the K and V vectors for *each token* in that sequence, for *each layer* of the model.\n",
            "    *   Instead of discarding these K and V vectors, they are stored in memory – this is the \"KV Cache\".\n",
            "    *   When the model needs to process the sequence *again* (e.g., to generate the next output token, or when you send the next message in a chat), it doesn't re-calculate the K and V vectors for the tokens it has *already* processed and whose K/V vectors are in the cache.\n",
            "    *   It only needs to calculate the K and V vectors for the *new* tokens (the latest user input, or the previously generated output token) and append them to the existing cache.\n",
            "    *   When computing the attention for the next token, it uses the K and V vectors from the *entire* sequence stored in the cache (old tokens + new tokens).\n",
            "\n",
            "3.  **Benefits of Context Caching:**\n",
            "    *   **Increased Speed / Lower Latency:** By avoiding the recomputation of K and V vectors for the entire previous context, the model can process new tokens much faster. This is crucial for real-time interactions like chatbots.\n",
            "    *   **Reduced Computation:** Fewer floating-point operations (FLOPs) are required, making inference more efficient.\n",
            "    *   **Improved Throughput:** On a server, faster individual requests can lead to serving more users concurrently (though memory is a limiting factor here).\n",
            "\n",
            "4.  **Drawbacks / Challenges:**\n",
            "    *   **Memory Usage:** Storing the K and V vectors for *every token* in the context, across *every layer* of the model, requires significant memory (usually GPU memory). The amount of memory needed scales with:\n",
            "        *   The context length (more tokens = more vectors).\n",
            "        *   The model size (more layers, larger hidden dimensions = larger vectors).\n",
            "        *   The batch size (processing multiple requests simultaneously, each needing its own cache).\n",
            "    *   **Context Length Limits:** Memory constraints are often the primary reason why LLMs have practical limits on their context window size. You simply run out of memory to store the KV cache for extremely long contexts.\n",
            "    *   **Cache Management:** Strategies are needed to manage the cache, especially when the context exceeds the maximum length (e.g., techniques like dropping the oldest tokens).\n",
            "\n",
            "**In Summary:**\n",
            "\n",
            "Context caching, primarily through **KV caching**, is a vital optimization technique in modern LLM serving. It prevents the model from redundant re-processing of the entire input history by storing the computed Key and Value vectors from the attention mechanism. This drastically speeds up inference for sequential tasks like text generation and conversational AI, at the cost of increased memory consumption which can limit the maximum usable context length.\n",
            "You should use context caching (specifically KV caching) primarily in scenarios where your Large Language Model (LLM) needs to:\n",
            "\n",
            "1.  **Generate text token by token (Autoregressive Decoding):** This is the most common use case. When an LLM generates a response, it does so one token at a time. To generate the *n*-th token, it needs to consider the *entire* sequence of tokens from the prompt plus the previously generated *n-1* tokens. KV caching stores the attention state (Keys and Values) for the preceding tokens, so the model only needs to compute and attend to the *new* token being generated, significantly speeding up the generation process.\n",
            "    *   **Examples:** Writing stories, composing emails, generating code, answering questions, summarizing documents (if the summary is generated token by token).\n",
            "\n",
            "2.  **Maintain context across multiple turns in a conversation:** In a chatbot or conversational AI, each new user message is added to the existing conversation history. To process the latest message and generate a relevant response, the model needs to consider the *entire* chat history up to that point. KV caching allows the model to store the attention state for the previous turns, so when a new message arrives, it only needs to process the new message and append its K/V vectors to the cache. This avoids reprocessing the entire chat history from scratch for every single turn.\n",
            "    *   **Examples:** Chatbots, virtual assistants, interactive dialogue systems.\n",
            "\n",
            "3.  **Process inputs that grow incrementally:** Any application where the input sequence is built up over time and subsequent processing steps need to refer to the entire accumulated sequence.\n",
            "\n",
            "**In short, use context caching whenever your LLM application involves:**\n",
            "\n",
            "*   **Sequential generation:** Producing output token by token.\n",
            "*   **Maintaining state over a growing sequence:** Like a conversation history.\n",
            "\n",
            "**When you *might not* need context caching (or its benefits are minimal):**\n",
            "\n",
            "*   **Single-turn tasks that don't involve sequential generation:**\n",
            "    *   Classification (e.g., sentiment analysis, topic classification)\n",
            "    *   Extraction (e.g., Named Entity Recognition, extracting specific data points)\n",
            "    *   Some forms of summarization (if not done via standard autoregressive decoding)\n",
            "    *   These tasks typically process the input once to produce a fixed output or distribution, and there's no subsequent dependency on newly generated tokens or a growing input history *within that specific request*.\n",
            "\n",
            "**However, even for tasks like summarization or question answering where the output is not a long generated text, if the *internal mechanism* of the model uses autoregressive decoding to produce the answer, KV caching *is* still used and beneficial for speeding up that internal generation step.**\n",
            "\n",
            "**The key trade-off:** Context caching provides speed but consumes significant memory (GPU memory) to store the Key and Value vectors. The amount of memory needed scales with context length and model size. This memory usage is often the primary limitation on the maximum effective context window size for LLMs.\n",
            "\n",
            "So, while technically possible to run without it for generation, **in practice, context caching is almost *always* enabled and essential for efficient inference in modern autoregressive LLMs for any task involving generating more than a few tokens or maintaining conversational context.** The performance penalty of *not* using it for these tasks is typically prohibitive.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'model' is initialized as before\n",
        "# Create content to cache (e.g., a large document or instructions)\n",
        "# Note: Caching currently supports specific models like gemini-1.5-flash-001\n",
        "# Check documentation for latest supported models for caching.\n",
        "# Use a model compatible with caching for this example if needed.\n",
        "model_name = \"gemini-1.5-flash-001\" # Example compatible model\n",
        "long_document = client.files.upload(file=\"/content/book.txt\") # Your large text content here\n",
        "cached_content = client.caches.create(\n",
        "    model=model_name,\n",
        "    config=genai.types.CreateCachedContentConfig(\n",
        "        contents=[long_document],\n",
        "        system_instruction=\"You are an expert analyzing transcripts of books.\",\n",
        "    ),\n",
        ")\n",
        "print(cached_content)\n",
        "\n",
        "# Use the cache in a request\n",
        "response = client.models.generate_content(\n",
        "    model=model_name,\n",
        "    contents=\"Please summarize this transcript\",\n",
        "    config=genai.types.GenerateContentConfig(cached_content=cached_content.name),\n",
        ")\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "KPjO7W1UOlP-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d84a7843-6c7e-44a6-c9df-87f35fb2054f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "name='cachedContents/m1rhajidp32sp28ofn01w9ohj3rqqp8xrwrt0syw' display_name='' model='models/gemini-1.5-flash-001' create_time=datetime.datetime(2025, 6, 11, 4, 43, 4, 400304, tzinfo=TzInfo(UTC)) update_time=datetime.datetime(2025, 6, 11, 4, 43, 4, 400304, tzinfo=TzInfo(UTC)) expire_time=datetime.datetime(2025, 6, 11, 5, 43, 3, 763367, tzinfo=TzInfo(UTC)) usage_metadata=CachedContentUsageMetadata(audio_duration_seconds=None, image_count=None, text_count=None, total_token_count=185317, video_duration_seconds=None)\n",
            "This transcript is the preface and the beginning of Samuel Butler's 1900 translation of Homer's *The Odyssey*. \n",
            "\n",
            "**Preface:**\n",
            "\n",
            "* Butler argues that *The Odyssey* was written by a young woman, Nausicaa, who lived in Trapani, Sicily. \n",
            "* He suggests that the poem is actually two poems merged together: (1) The Return of Ulysses and (2) The story of Penelope and the suitors.\n",
            "* Butler emphasizes that *The Odyssey* was written before 750 B.C. and likely before 1000 B.C., and that the author was demonstrably familiar with the *Iliad*.\n",
            "\n",
            "**Book I:**\n",
            "\n",
            "* The poem begins with the gods in council, discussing Ulysses' plight on the island of Calypso.\n",
            "* Minerva proposes to help Ulysses by going to Ithaca to encourage his son Telemachus to confront the suitors who are vying for his mother Penelope's hand.\n",
            "* Minerva disguises herself as Mentes, a Taphian leader, and visits Telemachus. She advises him to call the assembly, confront the suitors, and sail to Pylos and Sparta to seek news of his father. \n",
            "* Telemachus addresses the assembly, denouncing the suitors for their behavior and declaring his intention to seek news of his father.\n",
            "* The suitors respond with insults and threats, but Telemachus remains resolute and vows to embark on his journey. \n",
            "* Jove sends two eagles as a sign of Ulysses' imminent return.\n",
            "* Telemachus, encouraged by Minerva, prepares for his voyage and sets sail for Pylos.\n",
            "\n",
            "This sets the stage for the central themes of the poem: Ulysses' journey home, the suitors' attempts to claim Penelope, and Telemachus' growth and maturation. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Enable thinking\n",
        "model_name = \"gemini-2.5-flash-preview-04-17\"\n",
        "response = client.models.generate_content(\n",
        "    model=model_name,\n",
        "    contents=\"Plan the steps to write a blog post about context caching.\",\n",
        "    config=genai.types.GenerateContentConfig(\n",
        "    thinking_config=genai.types.ThinkingConfig(\n",
        "      include_thoughts=True\n",
        "    )\n",
        "  )\n",
        ")\n",
        "\n",
        "# Access thinking steps (if available)\n",
        "# if response.prompt_feedback.usage_metadata.thinking_steps:\n",
        "#     print(\"Thinking Steps:\")\n",
        "#     for step in response.prompt_feedback.usage_metadata.thinking_steps:\n",
        "#          print(step) # Inspect the reasoning process\n",
        "\n",
        "# Access final answer\n",
        "print(\"\\nFinal Answer:\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVslvpvvTOCE",
        "outputId": "cb4b6f36-bb28-4db5-b1ba-4788d55b4ff2"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Answer:\n",
            "Okay, here is a plan outlining the steps to write a blog post about context caching.\n",
            "\n",
            "**Blog Post Title Ideas (Choose one or refine):**\n",
            "\n",
            "*   Boost Performance: Understanding and Implementing Context Caching\n",
            "*   Context Caching: The Secret to Efficient Request Handling\n",
            "*   Go Beyond Basic Caching: Mastering Context-Specific Data\n",
            "*   Why You Need Context Caching in Your Applications\n",
            "\n",
            "**Target Audience:** Developers (Junior to Senior), System Architects, Performance Engineers.\n",
            "\n",
            "**Goal of the Post:** Explain what context caching is, why it's useful, where it's applied, how it's typically implemented, and the potential pitfalls to avoid.\n",
            "\n",
            "---\n",
            "\n",
            "**Phase 1: Planning & Research**\n",
            "\n",
            "1.  **Define Core Concepts:**\n",
            "    *   What is \"context\"? (e.g., a single web request, a background job execution, a user session, a transaction).\n",
            "    *   What is \"caching\" in general? (Storing frequently accessed data).\n",
            "    *   How do these combine to form \"context caching\"? (Storing data specifically for the duration of *that* context).\n",
            "    *   How does it differ from application-level caching or distributed caching? (Scope and lifespan).\n",
            "\n",
            "2.  **Identify Key Questions to Answer:**\n",
            "    *   What problem does context caching solve?\n",
            "    *   What are the benefits?\n",
            "    *   What are common use cases?\n",
            "    *   How is it typically implemented in different environments/languages (e.g., ThreadLocal in Java/C#, request scopes in web frameworks)?\n",
            "    *   What are the risks and challenges?\n",
            "    *   What are the best practices?\n",
            "\n",
            "3.  **Outline the Structure:** A standard blog post structure works well:\n",
            "    *   **Catchy Title:** Grab attention, state topic.\n",
            "    *   **Introduction:**\n",
            "        *   Hook: Start with a relatable problem (e.g., repeated database calls within a single request).\n",
            "        *   Introduce Context Caching as a solution.\n",
            "        *   Briefly define it.\n",
            "        *   State what the post will cover.\n",
            "    *   **What is Context Caching?**\n",
            "        *   Formal definition.\n",
            "        *   Explain the concept of \"context\" in this context.\n",
            "        *   Contrast with other caching types (application, distributed) - emphasize the *scope* and *lifespan*.\n",
            "        *   Simple example (e.g., caching the authenticated user object or a configuration setting per request).\n",
            "    *   **Why Use Context Caching? (Benefits)**\n",
            "        *   Performance (reduced redundant work).\n",
            "        *   Resource Efficiency (less load on database, external services, CPU).\n",
            "        *   Consistency within the context (ensure same data is used throughout).\n",
            "        *   Simplifies code (don't need to pass the data explicitly everywhere).\n",
            "    *   **Common Use Cases:**\n",
            "        *   Web Request Processing (User principal, Tenant ID, Correlation ID, request-specific configuration).\n",
            "        *   Multi-tenant Applications (Tenant context).\n",
            "        *   Background Job Execution (Job context, batch details).\n",
            "        *   Microservice Architectures (Propagating trace/security context).\n",
            "    *   **How is it Implemented? (Technical Details)**\n",
            "        *   **ThreadLocal:** Explain how it works (per-thread storage). Mention its prevalence and potential issues (thread pool reuse).\n",
            "        *   **Request Scopes/Context Objects:** How frameworks (Spring, ASP.NET Core, Node.js request context libraries) provide explicit mechanisms tied to request lifecycle. Dependency Injection context.\n",
            "        *   **Custom Context Objects:** Briefly mention how one might build their own system.\n",
            "        *   *Self-Correction:* Avoid getting *too* deep into framework-specific code unless planning a series; keep it conceptual or use simple pseudocode examples.\n",
            "    *   **Challenges and Pitfalls:**\n",
            "        *   **Memory Leaks:** (Especially with ThreadLocal if not cleaned up).\n",
            "        *   **Stale Data:** (Less common for short-lived contexts, but possible).\n",
            "        *   **Complexity:** Managing the lifecycle correctly.\n",
            "        *   **Testing:** Can be harder to test functions relying on implicit context.\n",
            "        *   **Concurrency Issues:** (If implementation isn't thread-safe, though ThreadLocal helps here).\n",
            "    *   **Best Practices:**\n",
            "        *   Strict lifecycle management (Set at start, clear at end).\n",
            "        *   Use framework-provided mechanisms where available.\n",
            "        *   Be mindful of thread pools (explicitly clear ThreadLocal).\n",
            "        *   Keep the cached data minimal and truly context-specific.\n",
            "        *   Document where context caching is used.\n",
            "    *   **Conclusion:**\n",
            "        *   Summarize key takeaways (benefits, importance of careful implementation).\n",
            "        *   Reinforce the value of context caching for performance and efficiency.\n",
            "        *   (Optional) Call to action (e.g., \"Share your experiences\").\n",
            "\n",
            "4.  **Gather Examples/Analogies:** Think of simple, clear examples or analogies to illustrate the concepts (e.g., carrying a shopping basket in a store - that's your context, everything you put in is cached for that trip).\n",
            "\n",
            "---\n",
            "\n",
            "**Phase 2: Writing the Draft**\n",
            "\n",
            "1.  **Write the Introduction:** Hook the reader and set the stage.\n",
            "2.  **Write the Core Sections (What, Why, Use Cases):** Explain the fundamental concepts and motivation.\n",
            "3.  **Write the Implementation Section:** Explain *how* it's done, using clear language and possibly simple pseudocode or conceptual diagrams. Address different approaches.\n",
            "4.  **Write the Challenges & Best Practices Section:** Provide warnings and guidance for successful implementation.\n",
            "5.  **Write the Conclusion:** Summarize and provide a final thought.\n",
            "6.  **Draft Code Snippets/Diagrams:** If including code, write it out clearly. If using diagrams, plan what they will depict (e.g., request flow with/without context caching, ThreadLocal concept).\n",
            "7.  **Review for Flow and Clarity:** Read through the draft. Do the sections transition smoothly? Is the language clear and accessible to the target audience? Is the technical explanation accurate?\n",
            "\n",
            "---\n",
            "\n",
            "**Phase 3: Refinement & Editing**\n",
            "\n",
            "1.  **Self-Edit:** Check for repetitive phrases, awkward sentences, and areas that need more explanation or simplification. Ensure consistent terminology.\n",
            "2.  **Technical Review:** If possible, have a colleague review the technical details for accuracy.\n",
            "3.  **Proofread:** Check for grammar, spelling, and punctuation errors.\n",
            "4.  **Add Visuals:** Integrate code snippets, diagrams, or other relevant images. Ensure they are well-formatted.\n",
            "5.  **Format for Blog:** Add headings (H2, H3), bullet points, bold text, code blocks according to the blog platform's formatting.\n",
            "6.  **Optimize for Search Engines (SEO):**\n",
            "    *   Include target keywords (\"context caching\", \"performance\", \"ThreadLocal\", \"request scope\", \"web development\") in the title, introduction, headings, and body.\n",
            "    *   Add a meta description.\n",
            "7.  **Final Read-Through:** Read the entire post one last time, preferably aloud, to catch any remaining errors or awkward phrasing.\n",
            "\n",
            "---\n",
            "\n",
            "This plan provides a solid framework for creating a comprehensive and informative blog post about context caching. Good luck!\n"
          ]
        }
      ]
    }
  ]
}