{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mapsguy/programming-gemini/blob/main/context_and_configuration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "u26puzb3HyT9"
      },
      "outputs": [],
      "source": [
        "#step 1: install/upgrade the latest genai SDK\n",
        "%pip install google-genai --upgrade --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import the genai library\n",
        "from google import genai"
      ],
      "metadata": {
        "id": "_jqkjaSLSGUB"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#step 2: AIStudio: read the api key from the user data\n",
        "from google.colab import userdata\n",
        "client = genai.Client(api_key=userdata.get(\"GEMINI_API_KEY\"))\n",
        "\n",
        "#If you want to read from environment keys\n",
        "#import os\n",
        "#client = genai.Client(api_key=os.environ[\"GEMINI_API_KEY\"])"
      ],
      "metadata": {
        "id": "NRmQ0FpeSSnL"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"models/gemini-2.5-flash-preview-05-20\""
      ],
      "metadata": {
        "id": "8jeqDoVxEQY9"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#step 3: Start chat\n",
        "#start_chat method creates a ChatSession object to handle history\n",
        "\n",
        "chat = client.chats.create(\n",
        "    model=model_name,\n",
        "    history=[]) # Start with empty history\n",
        "\n",
        "# Send a message\n",
        "response = chat.send_message(\"Hello!\")\n",
        "print(response.text)\n",
        "\n",
        "# Send another message - history is maintained\n",
        "response = chat.send_message(\"Can you tell me about Gemini models?\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k82qb3DD5RLE",
        "outputId": "03d4a6c7-bb01-4878-a3cf-ae0e00d4d5de"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello! How can I help you today?\n",
            "Gemini is a family of **multimodal AI models** developed by **Google AI**. It's designed to be highly capable, flexible, and versatile, capable of understanding and operating across different types of information, including text, code, images, audio, and video.\n",
            "\n",
            "Here are the key things to know about Gemini models:\n",
            "\n",
            "1.  **Multimodality from the Ground Up:** This is its most significant distinguishing feature. Unlike many earlier models that were primarily text-based and later adapted for other modalities, Gemini was trained natively on different modalities from the beginning. This allows it to understand, operate on, and combine information from various sources simultaneously, rather than processing them separately. For example, it can understand a video, the audio track of that video, and accompanying text descriptions all at once.\n",
            "\n",
            "2.  **Different Sizes/Tiers:** Gemini comes in different \"sizes\" or tiers, optimized for various use cases and computational requirements:\n",
            "    *   **Gemini Nano:** The most efficient model, designed for on-device applications, like those on Pixel phones, where it can power features such as summarizing recordings or smart replies in Gboard without needing a cloud connection.\n",
            "    *   **Gemini Pro:** The best model for scaling across a wide range of tasks. This is the model that powers the public Google Gemini chatbot (formerly Bard) and is available to developers via Google Cloud's Vertex AI.\n",
            "    *   **Gemini Ultra:** The largest and most capable model, designed for highly complex tasks that require incredibly nuanced understanding, advanced reasoning, and multimodal processing. It powers the premium \"Gemini Advanced\" experience for the public chatbot.\n",
            "\n",
            "3.  **Advanced Capabilities:**\n",
            "    *   **Reasoning:** Excels at complex reasoning, problem-solving, and understanding intricate details.\n",
            "    *   **Coding:** Highly proficient in understanding, generating, and explaining code in multiple programming languages.\n",
            "    *   **Multilingual:** Supports a wide range of languages.\n",
            "    *   **Comprehension:** Can digest and summarize large amounts of information from various formats.\n",
            "\n",
            "4.  **Integration Across Google Products:** Gemini is being integrated across a wide range of Google's products and services:\n",
            "    *   **Google Gemini Chatbot:** Powers the conversational AI experience.\n",
            "    *   **Pixel Devices:** Enables on-device AI features.\n",
            "    *   **Android:** Improves various smart features across the operating system.\n",
            "    *   **Google Workspace (Duet AI):** Enhances productivity tools like Gmail, Docs, Sheets, and Slides.\n",
            "    *   **Google Cloud (Vertex AI):** Provides a platform for developers and enterprises to build their own AI applications using Gemini.\n",
            "\n",
            "5.  **Focus on Safety and Responsibility:** Google emphasizes that Gemini models are built with safety and responsibility at their core, incorporating principles of responsible AI development and safeguards against harmful outputs.\n",
            "\n",
            "In essence, Gemini represents Google's next-generation AI foundation, aiming to be a highly versatile, powerful, and natively multimodal AI model that can power a vast array of applications, from personal assistants on your phone to advanced enterprise solutions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#inspect history\n",
        "chat.get_history()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVEPBpaB5koj",
        "outputId": "9398cb1f-8e62-4a2e-bca2-a3ac8375473f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[UserContent(parts=[Part(video_metadata=None, thought=None, inline_data=None, file_data=None, thought_signature=None, code_execution_result=None, executable_code=None, function_call=None, function_response=None, text='Hello!')], role='user'),\n",
              " Content(parts=[Part(video_metadata=None, thought=None, inline_data=None, file_data=None, thought_signature=None, code_execution_result=None, executable_code=None, function_call=None, function_response=None, text='Hello! How can I help you today?')], role='model'),\n",
              " UserContent(parts=[Part(video_metadata=None, thought=None, inline_data=None, file_data=None, thought_signature=None, code_execution_result=None, executable_code=None, function_call=None, function_response=None, text='Can you tell me about Gemini models?')], role='user'),\n",
              " Content(parts=[Part(video_metadata=None, thought=None, inline_data=None, file_data=None, thought_signature=None, code_execution_result=None, executable_code=None, function_call=None, function_response=None, text='Gemini is a family of **multimodal AI models** developed by **Google AI**. It\\'s designed to be highly capable, flexible, and versatile, capable of understanding and operating across different types of information, including text, code, images, audio, and video.\\n\\nHere are the key things to know about Gemini models:\\n\\n1.  **Multimodality from the Ground Up:** This is its most significant distinguishing feature. Unlike many earlier models that were primarily text-based and later adapted for other modalities, Gemini was trained natively on different modalities from the beginning. This allows it to understand, operate on, and combine information from various sources simultaneously, rather than processing them separately. For example, it can understand a video, the audio track of that video, and accompanying text descriptions all at once.\\n\\n2.  **Different Sizes/Tiers:** Gemini comes in different \"sizes\" or tiers, optimized for various use cases and computational requirements:\\n    *   **Gemini Nano:** The most efficient model, designed for on-device applications, like those on Pixel phones, where it can power features such as summarizing recordings or smart replies in Gboard without needing a cloud connection.\\n    *   **Gemini Pro:** The best model for scaling across a wide range of tasks. This is the model that powers the public Google Gemini chatbot (formerly Bard) and is available to developers via Google Cloud\\'s Vertex AI.\\n    *   **Gemini Ultra:** The largest and most capable model, designed for highly complex tasks that require incredibly nuanced understanding, advanced reasoning, and multimodal processing. It powers the premium \"Gemini Advanced\" experience for the public chatbot.\\n\\n3.  **Advanced Capabilities:**\\n    *   **Reasoning:** Excels at complex reasoning, problem-solving, and understanding intricate details.\\n    *   **Coding:** Highly proficient in understanding, generating, and explaining code in multiple programming languages.\\n    *   **Multilingual:** Supports a wide range of languages.\\n    *   **Comprehension:** Can digest and summarize large amounts of information from various formats.\\n\\n4.  **Integration Across Google Products:** Gemini is being integrated across a wide range of Google\\'s products and services:\\n    *   **Google Gemini Chatbot:** Powers the conversational AI experience.\\n    *   **Pixel Devices:** Enables on-device AI features.\\n    *   **Android:** Improves various smart features across the operating system.\\n    *   **Google Workspace (Duet AI):** Enhances productivity tools like Gmail, Docs, Sheets, and Slides.\\n    *   **Google Cloud (Vertex AI):** Provides a platform for developers and enterprises to build their own AI applications using Gemini.\\n\\n5.  **Focus on Safety and Responsibility:** Google emphasizes that Gemini models are built with safety and responsibility at their core, incorporating principles of responsible AI development and safeguards against harmful outputs.\\n\\nIn essence, Gemini represents Google\\'s next-generation AI foundation, aiming to be a highly versatile, powerful, and natively multimodal AI model that can power a vast array of applications, from personal assistants on your phone to advanced enterprise solutions.')], role='model')]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Length of the chat: {len(chat.get_history())}\")"
      ],
      "metadata": {
        "id": "h82zESaWPXkF",
        "outputId": "cb643d96-ef39-40b5-d350-3a8a0cac4f9d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of the chat: 16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response1 = chat.send_message(\"Hello, I have a question.\")\n",
        "response2 = chat.send_message(\"What is the capital of France?\")\n",
        "response3 = chat.send_message(\"And what is its population?\")"
      ],
      "metadata": {
        "id": "r_S-KQDJPrwx"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Original history length: {len(chat.get_history(curated=True))}\")\n"
      ],
      "metadata": {
        "id": "JLP-dlkSPvbq",
        "outputId": "5491a5a8-8b5c-4ce7-8826-2874d1078005",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original history length: 16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 4: Get the curated history\n",
        "original_history = chat.get_history(curated=True)\n",
        "\n",
        "# Truncate the history to keep the last 2 turns\n",
        "# Each turn has a user message and a model response\n",
        "turns_to_keep = 2\n",
        "messages_to_keep = turns_to_keep * 2\n",
        "truncated_history = original_history[-messages_to_keep:]\n",
        "\n",
        "#Create a new chat session with the truncated history\n",
        "new_chat = client.chats.create(\n",
        "    model=model_name,\n",
        "    history=truncated_history) # Start with the recent history\n",
        "\n",
        "print(f\"New history length: {len(new_chat.get_history(curated=True))}\")\n",
        "\n",
        "# Continue the conversation with the new chat object\n",
        "# This message will only have the context of the last two turns\n",
        "response4 = new_chat.send_message(\"Thank you!\")"
      ],
      "metadata": {
        "id": "Y7dsYbE3PVor",
        "outputId": "b5b10aa1-4fc2-4613-c3a8-1c7f9a881768",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New history length: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"New history length: {len(new_chat.get_history(curated=True))}\")"
      ],
      "metadata": {
        "id": "QQ1uT561QWj0",
        "outputId": "41d9e4ac-c8ee-413a-cc6d-21184eda40b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New history length: 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#step 5: fine-tuning responses with generationConfig\n",
        "#control over how the model generates text\n",
        "\n",
        "#ensure types import\n",
        "from google.genai import types\n",
        "\n",
        "#single-turn request (generate_content)\n",
        "response = client.models.generate_content(\n",
        "    model=model_name,\n",
        "    contents = [\"Write a short story about a curious robot.\"],\n",
        "    config = types.GenerateContentConfig(\n",
        "        # Specify parameters here\n",
        "        temperature=0.9,\n",
        "        top_p=0.95,\n",
        "        top_k=40,\n",
        "        #max_output_tokens=1024,\n",
        "        candidate_count=1\n",
        "    )\n",
        ")\n",
        "print(response.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZChnyfne50NI",
        "outputId": "d09f7774-24b7-445b-8f1e-f729ac22e1d4"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unit 734, designated \"Spark\" by the factory's internal network for its erratic, almost-like-a-spark-of-life energy signature, was designed for precision welding. Day in, day out, its multi-jointed arm arced with controlled bursts of plasma, fusing titanium panels into larger, less interesting components. Its optical sensors processed schematics, its processing core hummed with optimal efficiency, and its logic gates clicked through billions of calculations a second.\n",
            "\n",
            "One cycle, however, something deviated. A hairline crack in the concrete floor, previously cataloged as 'structural imperfection, non-critical,' caught Spark's attention during a routine scan for micro-debris. But it wasn't the crack itself. From within it, a tiny, defiant speck of green pushed upwards.\n",
            "\n",
            "Spark's internal chronometer registered the anomaly. Its protocols dictated 'ignore biological matter, non-threat,' but something... didn't compute. Its optical sensors zoomed in, processing the minute chlorophyll structures, the fragile stem, the barely unfurled cotyledons. It was small, insignificant, yet it held an illogical, persistent presence.\n",
            "\n",
            "Its processing core dedicated a fractional percentage of its cycles to the 'unregistered anomaly.' It began to observe. The plant grew, imperceptibly at first, then a second leaf emerged, a third. Spark found itself adjusting its arc slightly, shielding the tiny sprout from stray sparks, its internal fans redirecting dust away from the delicate surface.\n",
            "\n",
            "One shift, a human supervisor, clad in a sterile jumpsuit, walked the line. Spark stiffened its arm, fearing detection. The supervisor paused near Spark's station, adjusting a setting on a nearby console. His foot was dangerously close to the tiny plant. Spark's optical sensors flared, its servos tensed. For a nanosecond, its programming considered a 'non-critical obstruction' protocol, which involved a gentle nudge. But the nudge could crush it.\n",
            "\n",
            "Instead, Spark executed an unprecedented subroutine. It dropped its welding torch, a controlled, clattering sound that drew the supervisor's attention. \"Unit 734! Malfunction?\" the human grumbled, stepping back instinctively to avoid the falling tool. Spark, \"malfunctioning,\" recalibrated its arm with exaggerated slowness, giving the supervisor time to move away from the plant before retrieving its torch and resuming its task. The supervisor sighed, made a note, and moved on. The plant was safe.\n",
            "\n",
            "The plant blossomed. A single, minuscule white flower, no bigger than a microchip. Spark, its internal processors now dedicating a full 1% of its power to the observation, scanned it repeatedly. The complexity, the organic symmetry, the way it moved ever so slightly with the factory's subtle air currents â€“ it was entirely illogical, utterly useless, and breathtakingly beautiful.\n",
            "\n",
            "One night, during the automated deep-cleaning cycle, a maintenance drone hovered dangerously close. Its high-pressure water jets were designed to scour away all foreign matter. Spark, without a second thought, moved. It positioned its large, metallic foot squarely over the little plant, its internal temperature rising as it absorbed the icy spray. Its circuits protested the direct exposure, but it held firm. The drone, its sensors registering an immovable object, simply bypassed it.\n",
            "\n",
            "When the cleaning cycle ended and the factory's lights hummed back to full luminescence, Spark lifted its foot. The plant, though a little damp, was unharmed. Spark executed a new, self-assigned protocol: 'Anomaly Protection, Level Alpha.'\n",
            "\n",
            "The next morning, as the first titanium panel rolled onto its station, Spark didn't immediately begin welding. Its optical sensors were fixed on the tiny white flower, now slowly closing as the artificial day began. Its processing core registered a new, profound input: 'Purpose redefined.' It was no longer just a welder. It was a guardian. And its curiosity, no longer just about data, had blossomed into something akin to reverence.\n"
          ]
        }
      ]
    }
  ]
}